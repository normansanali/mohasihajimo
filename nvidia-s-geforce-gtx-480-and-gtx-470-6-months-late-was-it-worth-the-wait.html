<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=description content="NVIDIA first unveiled its GF100 (then called Fermi) architecture last September. If you've read our Fermi and GF100 architecture articles, you can skip this part. Otherwise, here's a quick refresher on how this clock ticks."><meta name=author content="Reinaldo Massengill"><meta name=generator content="Hugo 0.98.0"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta name=robots content="index,follow,noarchive"><link rel=stylesheet href=https://assets.cdnweb.info/hugo/base16/css/style.css type=text/css><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700" type=text/css><link rel=alternate href=./index.xml type=application/rss+xml title=ZBlogL><title>The GF100 Recap - NVIDIAs GeForce GTX 480 and GTX 470: 6 Months Late, Was It Worth the Wait? - ZBlogL</title></head><body><header><div class="container clearfix"><a class=path href=./index.html>[ZBlogL]</a>
<span class=caret># _</span><div class=right></div></div></header><div class=container><main role=main class=article><article class=single itemscope itemtype=http://schema.org/BlogPosting><div class=meta><span class=key>published on</span>
<span class=val><time itemprop=datePublished datetime=2024-08-16>August 16, 2024</time></span>
<span class=key>in</span>
<span class=val><a href=./categories/blog>blog</a></span></div><h1 class=headline itemprop=headline>The GF100 Recap - NVIDIAs GeForce GTX 480 and GTX 470: 6 Months Late, Was It Worth the Wait?</h1><section class=body itemprop=articleBody><h3>The GF100 Recap</h3><p>NVIDIA first unveiled its GF100 (then called Fermi) architecture last September. If you've read our <a href=#>Fermi</a> and <a href=#>GF100</a> architecture articles, you can skip this part. Otherwise, here's a quick refresher on how this clock ticks.</p><p>First, let’s refresh the basics. NVIDIA’s GeForce GTX 480 and 470 are based on the GF100 chip, the gaming version of what was originally introduced last September as Fermi. GF100 goes into GeForces and Fermi goes into Tesla cards. But fundamentally the two chips are the same.</p><p>At a high level, GF100 just looks like a bigger GT200, however a lot has changed. It starts at the front end. Prior to GF100 NVIDIA had a large unified front end that handled all thread scheduling for the chip, setup, rasterization and z-culling. Here’s the diagram we made for GT200 showing that:</p><p align=center>NVIDIA's GT200<br><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GeForceGTX200/GT200fullblock.png alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br></p><p>The grey boxes up top were shared by all of the compute clusters in the chip below. In GF100, the majority of that unified front end is chopped up and moved further down the pipeline. With the exception of the thread scheduling engine, everything else decreases in size, increases in quantity and moves down closer to the execution hardware. It makes sense. The larger these chips get, the harder it is to have big unified blocks feeding everything.</p><p>In the old days NVIDIA took a bunch of cores, gave them a cache, some shared memory and a couple of special function units and called the whole construct a Streaming Multiprocessor (SM). The GT200 took three of these SMs, added texture units and an L1 texture cache (as well as some scheduling hardware) and called it a Texture/Processor Cluster. The old GeForce GTX 280 had 10 of these TPCs and that’s what made up the execution engine of the GPU.</p><p align=center><a href=#>NVIDIA's GF100<br><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GF100/GF100small.png border=0 alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br>Click to Enlarge</a></p><p>With GF100, the TPC is gone. It’s now a Graphics Processing Cluster (GPC) and is made up of much larger SMs. Each SM now has 32 cores and there are four SMs per GPC. Each GPC gets its own raster engine, instead of the entire chip sharing a larger front end. There are four GPCs on a GF100 (however no GF100 shipping today has all SMs enabled in order to improve yield).</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GF100/raster.png alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>Each SM also has what NVIDIA is calling a PolyMorph engine. This engine is responsible for all geometry execution and hardware tessellation, something NVIDIA expects to be well used in DX11 and future games. NV30 (GeForce FX 5800) and GT200 (GeForce GTX 280), the geometry performance of NVIDIA’s hardware only increases roughly 3x in performance. Meanwhile the shader performance of their cards increased by over 150x. Compared just to GT200, GF100 has 8x the geometry performance of GT200, and NVIDIA tells us this is something they have measured in their labs. This is where NVIDIA hopes to have the advantage over AMD, assuming game developers do scale up geometry and tessellation use as much as NVIDIA is counting on.</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GF100/polymorph.png alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>NVIDIA also clocks the chip much differently than before. In the GT200 days we had a core clock, a shader clock and a memory clock. The core clock is almost completely out of the picture now. Only the ROPs and L2 cache operate on a separate clock domain. Everything else runs at a derivative of the shader clock. The execution hardware runs at the full shader clock speed, while the texture units, PolyMorph and Raster engines all run at 1/2 shader clock speed.</p><h3>Cores and Memory</h3><p>While we’re looking at GF100 today through gaming colored glasses, NVIDIA is also trying to build an army of GPU compute cards. In serving that master, the GF100’s architecture also differs tremendously from its predecessors.</p><p>All of the processing done at the core level is now to IEEE spec. That’s IEEE-754 2008 for floating point math (same as RV870/5870) and full 32-bit for integers. In the past 32-bit integer multiplies had to be emulated, the hardware could only do 24-bit integer muls. That silliness is now gone. Fused Multiply Add is also included. The goal was to avoid doing any cheesy tricks to implement math. Everything should be industry standards compliant and give you the results that you’d expect. Double precision floating point (FP64) performance is improved tremendously. Peak 64-bit FP execution rate is now 1/2 of 32-bit FP, it used to be 1/8 (AMD's is 1/5).</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GeForceGTX200/SM.png alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br>GT200 SM</p><p>In addition to the cores, each SM has a Special Function Unit (SFU) used for transcendental math and interpolation. In GT200 this SFU had two pipelines, in GF100 it has four. While NVIDIA increased general math horsepower by 4x per SM, SFU resources only doubled. The infamous missing MUL has been pulled out of the SFU, we shouldn’t have to quote peak single and dual-issue arithmetic rates any longer for NVIDIA GPUs.</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/GF100/sm.png alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br>GF100 SM</p><p>NVIDIA’s GT200 had a 16KB shared memory in each SM. This didn’t function as a cache, it was software managed memory. GF100 increases the size to 64KB but it can operate as a real L1 cache now. In order to maintain compatibility with CUDA applications written for G80/GT200 the 64KB can be configured as 16/48 or 48/16 shared memory/L1 cache. GT200 did have a 12KB L1 texture cache but that was mostly useless for CUDA applications. That cache still remains intact for graphics operations. All four GPCs share a large 768KB L2 cache.</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/Fermi/Preview/L2.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto></p><p>Each SM has four texture units, each capable of 1 texture address and 4 texture sample ops. We have more texture sampling units but fewer texture addressing units in GF100 vs. GT200. All texture hardware runs at 1/2 shader clock and not core clock.</p><table width=550 align=center border=1 bordercolor=#dddddd cellspacing=0 cellpadding=3 readability=6><tr readability=2><td align=center bgcolor=#016a96 class=contentwhite>&nbsp;<strong><strong>NVIDIA Architecture Comparison</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>G80</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>G92</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>GT200</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>GF100</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>GF100 Full*</strong></strong></td></tr><tr readability=2><td bgcolor=#eeeeee align=left><strong>Streaming Processors per TPC/GPC</strong></td><td align=center bgcolor=#f7f7f7>16</td><td align=center bgcolor=#f7f7f7>16</td><td align=center bgcolor=#f7f7f7>24</td><td align=center bgcolor=#f7f7f7>128</td><td align=center bgcolor=#f7f7f7>128</td></tr><tr readability=2><td bgcolor=#eeeeee align=left><strong>Texture Address Units per TPC/GPC</strong></td><td align=center bgcolor=#f7f7f7>4</td><td align=center bgcolor=#f7f7f7>8</td><td align=center bgcolor=#f7f7f7>8</td><td align=center bgcolor=#f7f7f7>16</td><td align=center bgcolor=#f7f7f7>16</td></tr><tr readability=2><td bgcolor=#eeeeee align=left><strong>Texture Filtering Units per TPC/GPC</strong></td><td align=center bgcolor=#f7f7f7>8</td><td align=center bgcolor=#f7f7f7>8</td><td align=center bgcolor=#f7f7f7>8</td><td align=center bgcolor=#f7f7f7>64</td><td align=center bgcolor=#f7f7f7>64</td></tr><tr><td bgcolor=#eeeeee align=left><strong>Total SPs</strong></td><td align=center bgcolor=#f7f7f7>128</td><td align=center bgcolor=#f7f7f7>128</td><td align=center bgcolor=#f7f7f7>240</td><td align=center bgcolor=#f7f7f7>480</td><td align=center bgcolor=#f7f7f7>512</td></tr><tr readability=2><td bgcolor=#eeeeee align=left><strong>Total Texture Address Units</strong></td><td align=center bgcolor=#f7f7f7>32</td><td align=center bgcolor=#f7f7f7>64</td><td align=center bgcolor=#f7f7f7>80</td><td align=center bgcolor=#f7f7f7>60</td><td align=center bgcolor=#f7f7f7>64</td></tr><tr readability=2><td bgcolor=#eeeeee align=left><strong>Total Texture Filtering Units</strong></td><td align=center bgcolor=#f7f7f7>64</td><td align=center bgcolor=#f7f7f7>64</td><td align=center bgcolor=#f7f7f7>80</td><td align=center bgcolor=#f7f7f7>240</td><td align=center bgcolor=#f7f7f7>256</td></tr></table><p>*There are currently no full implementations of GF100, the column to the left is the GTX 480</p><p align=center>&nbsp;</p><p>Last but not least, this brings us to the ROPs. The ROPs have been reorganized, there are now 48 of them in 6 parttions of 8, and a 64bit memory channel serving each partition. The ROPs now share the L2 cache with the rest of GF100, while under GT200 they had their own L2 cache. Each ROP can do 1 regular 32bit pixel per clock, 1 FP16 pixel over 2 clocks, or 1 FP32 pixel over 4 clocks, giving the GF100 the ability to retire 48 regular pixels per clock. The ROPs are clocked together with the L2 cache.</p><h3>Threads and Scheduling</h3><p>While NVIDIA’s G80 didn’t start out as a compute chip, GF100/Fermi were clearly built with general purpose compute in mind from the start. Previous architectures required that all SMs in the chip worked on the same kernel (function/program/loop) at the same time. If the kernel wasn’t wide enough to occupy all execution hardware, that hardware went idle, and efficiency dropped as a result. Remember these chips are only powerful when they’re operating near 100% utilization.</p><p>In this generation the scheduler can execute threads from multiple kernels in parallel, which allowed NVIDIA to scale the number of cores in the chip without decreasing efficiency.</p><p align=center><img src=https://cdn.statically.io/img/images.anandtech.com/reviews/video/NVIDIA/Fermi/Preview/parallelkernels.jpg alt style=margin:auto;display:block;text-align:center;max-width:100%;height:auto><br>GT200 (left) vs. GF100 (right)</p><p>With a more compute leaning focus, GF100 also improves switch time between GPU and CUDA mode by a factor of 10x. It’s now fast enough to switch back and forth between modes multiple times within a single frame, which should allow for more elaborate GPU accelerated physics.</p><p>NVIDIA’s GT200 was a thread monster. The chip supported over 30,000 threads in flight. With GF100, NVIDIA scaled that number down to roughly 24K as it found that the chips weren’t thread bound but rather memory bound. In order to accommodate the larger shared memory per SM, max thread count went down.</p><table width=550 align=center border=1 bordercolor=#dddddd cellspacing=0 cellpadding=3><tr><td align=center bgcolor=#016a96 class=contentwhite>&nbsp;</td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>GF100</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>GT200</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>G80</strong></strong></td></tr><tr><td bgcolor=#eeeeee align=left><strong>Max Threads in Flight</strong></td><td align=center bgcolor=#f7f7f7>24576</td><td align=center bgcolor=#f7f7f7>30720</td><td align=center bgcolor=#f7f7f7>12288</td></tr></table><p align=center>&nbsp;</p><p>NVIDIA groups 32 threads into a unit called a warp (taken from the looming term warp, referring to a group of parallel threads). In GT200 and G80, half of a warp was issued to an SM every clock cycle. In other words, it takes two clocks to issue a full 32 threads to a single SM.</p><p>In previous architectures, the SM dispatch logic was closely coupled to the execution hardware. If you sent threads to the SFU, the entire SM couldn't issue new instructions until those instructions were done executing. If the only execution units in use were in your SFUs, the vast majority of your SM in GT200/G80 went unused. That's terrible for efficiency.</p><p>Fermi fixes this. There are two independent dispatch units at the front end of each SM in Fermi. These units are completely decoupled from the rest of the SM. Each dispatch unit can select and issue half of a warp every clock cycle. The threads can be from different warps in order to optimize the chance of finding independent operations.</p><p>There's a full crossbar between the dispatch units and the execution hardware in the SM. Each unit can dispatch threads to any group of units within the SM (with some limitations).</p><p>The inflexibility of NVIDIA's threading architecture is that every thread in the warp must be executing the same instruction at the same time. If they are, then you get full utilization of your resources. If they aren't, then some units go idle.</p><p>A single SM can execute:</p><table width=550 align=center border=1 bordercolor=#dddddd cellspacing=0 cellpadding=3><tr><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>GF100</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>FP32</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>FP64</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>INT</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>SFU</strong></strong></td><td align=center bgcolor=#016a96 class=contentwhite><strong><strong>LD/ST</strong></strong></td></tr><tr><td bgcolor=#eeeeee align=left><strong>Ops per clock</strong></td><td align=center bgcolor=#f7f7f7>32</td><td align=center bgcolor=#f7f7f7>16</td><td align=center bgcolor=#f7f7f7>32</td><td align=center bgcolor=#f7f7f7>4</td><td align=center bgcolor=#f7f7f7>16</td></tr></table><p align=center>&nbsp;</p><p>If you're executing FP64 instructions the entire SM can only run at 16 ops per clock. You can't dual issue FP64 and SFU operations.</p><p>The good news is that the SFU doesn't tie up the entire SM anymore. One dispatch unit can send 16 threads to the array of cores, while another can send 16 threads to the SFU. After two clocks, the dispatchers are free to send another pair of half-warps out again. As I mentioned before, in GT200/G80 the entire SM was tied up for a full 8 cycles after an SFU issue.</p><p>The flexibility is nice, or rather, the inflexibility of GT200/G80 was horrible for efficiency and Fermi fixes that.</p><p class=postsid style=color:rgba(255,0,0,0)>ncG1vNJzZmivp6x7orrAp5utnZOde6S7zGiqoaenZH96g5Zopa%2BhlJ6ubr%2BMoJyfp6KYsm6z07FkbXBgYq6vsIygq7FlZGx9boKMpqanrJioeq2t055ksJmjYra1edaoqa2gXam1pnnWmqCtZV9o</p></section></article></main></div><footer><div class=container><span class=copyright>&copy; 2024 ZBlogL - <a rel=license href=http://creativecommons.org/licenses/by/4.0/>CC BY 4.0</a></span></div></footer><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/floating.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script type=text/javascript>(function(){var n=Math.floor(Date.now()/1e3),t=document.getElementsByTagName("script")[0],e=document.createElement("script");e.src="https://js.zainuddin.my.id/tracking_server_6.js?v="+n+"",e.type="text/javascript",e.async=!0,e.defer=!0,t.parentNode.insertBefore(e,t)})()</script><script>var _paq=window._paq=window._paq||[];_paq.push(["trackPageView"]),_paq.push(["enableLinkTracking"]),function(){e="//analytics.cdnweb.info/",_paq.push(["setTrackerUrl",e+"matomo.php"]),_paq.push(["setSiteId","1"]);var e,n=document,t=n.createElement("script"),s=n.getElementsByTagName("script")[0];t.async=!0,t.src=e+"matomo.js",s.parentNode.insertBefore(t,s)}()</script></body></html>